# Meeting 2024-10-21 (Andy, Alex, Francis)

- Education
- External Model/Document Fact Checker and such
  - ensemble of models, mix of experts, etc
    - "better" consilient output, etc
      - resembling Random Forests
        - [Curth, Alicia, Alan Jeffares, and Mihaela van der Schaar. "Why do Random Forests Work? Understanding Tree Ensembles as Self-Regularizing Adaptive Smoothers" arXiv preprint (2024)](https://arxiv.org/abs/2402.01502)
    - spot outliers
  - classic AI, classic LM
    - BERT (fill in poor areas with good BERT/RETRO output)
    - semantic analysis (spot similar text, changes in meaning, etc that string match can't)
  - "defence in depth" approach to fact-checking
    - perhaps the input is treated as "auto-adversarial", considering something like an LLM as a danger to itself (Markov-chain / randomness prevents robust prediction of factual details, etc)
  - fuzzing the inputs, testing multiple outputs (top-k rather than temperature), testing multiple chains/permutations of outputs, and doing all this across an ensemble of models to gauge agreement, lack of agreement, distribution of divergent interpretations, surfacing of relevant details/facts, and so forth
  - an important aspect is to check we "stay on topic"
  - by checking at different layers (hierarchical/pyramidal checking) at different intervals (low-latency / tight-loop token/word checking, mid-latency sentence checking, high-latency paragraph/response checking) we can check smaller semantic/factual details as well as overall "messaging" (preventing semantic confusion or negation/redirection from the accurate/correct statements)
- Citation, metadata recall/retrieval
  - for RAG style document/input focusing
  - for RETRO style fact/token embedding
  - vector search (nearest vector to doc tokens/vectors or metadata layer)
    - can grab sources
    - can grab similar/nearby
    - can grab semantic neighbours (fuzzed?)
      - negated/tweaked for alternatives (counterfactuals, etc)
      - local landscape (explainability)
